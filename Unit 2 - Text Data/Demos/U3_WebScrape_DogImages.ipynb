{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d61d65f-23b1-445b-8544-500b5c6e0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42da8a-fbc8-4c22-9498-f8a741310d3d",
   "metadata": {},
   "source": [
    "# Scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af222ab7-fc8b-49a5-a323-6503d2a63c1a",
   "metadata": {},
   "source": [
    "### Read first webpage with list of computer scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d2816-7e14-4c98-b8b2-5276539b2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read webpage and parse with bs4\n",
    "base_url = 'https://en.wikipedia.org/'\n",
    "list_url = 'wiki/List_of_dog_breeds'\n",
    "url = base_url + list_url\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Get names of scientists and links to their webpages\n",
    "links = {}\n",
    "div = soup.find('div', class_=\"mw-content-ltr mw-parser-output\")\n",
    "for ul in div.find_all('ul')[:5]:\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        a = li.find(\"a\")\n",
    "        href = a['href']\n",
    "        links[a.text] = href\n",
    "        #print(a.text)\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04b857-4ff3-4daf-af05-d722ef360837",
   "metadata": {},
   "source": [
    "# Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c23faf5-52f8-474e-99fb-e9ee1639fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Affenpinscher\n",
      "2 Afghan Hound\n",
      "3 Africanis\n",
      "4 Airedale Terrier\n",
      "5 Akita\n",
      "6 American Bulldog\n",
      "7 American Bully\n",
      "8 American English Coonhound\n",
      "9 American Eskimo Dog\n",
      "10 American Foxhound\n",
      "11 American Leopard Hound\n",
      "12 American Staffordshire Terrier\n",
      "13 Anglo-Français de Petite Vénerie\n",
      "14 Armant\n",
      "15 Australian Cattle Dog\n",
      "16 Australian Shepherd\n",
      "17 Australian Terrier\n",
      "18 Azawakh\n",
      "19 Basenji\n",
      "20 Basset Hound\n",
      "21 Beagle\n",
      "22 Beauceron\n",
      "23 Belgian Shepherd\n",
      "24 Biewer Terrier\n",
      "25 Borzoi\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"Dog Breeds/\"\n",
    "\n",
    "# dictionary of breeds and filenames\n",
    "breed_info = {}\n",
    "\n",
    "# Number of pages to scrape\n",
    "count = 0\n",
    "\n",
    "for breed_name, link in links.items():\n",
    "    # skip any failed requests\n",
    "    try:\n",
    "        # Read individual webpage\n",
    "        url = base_url + link\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', class_=\"infobox\")\n",
    "        image = table.find(\"img\")\n",
    "    \n",
    "        # dont run if image is empty\n",
    "        if image:\n",
    "            url_img = \"https:\" + image[\"src\"]\n",
    "            image_response = requests.get(url_img, timeout=10)\n",
    "            \n",
    "            # raise error if bad status\n",
    "            image_response.raise_for_status()\n",
    "    \n",
    "            # create filepath\n",
    "            img_filename = \"_\".join(breed_name.split(\" \"))\n",
    "            img_filename = re.sub(r\"\\W\", \"\", img_filename) + \".png\"\n",
    "            img_filepath = img_dir + img_filename\n",
    "    \n",
    "            # download and save image\n",
    "            img = Image.open(BytesIO(image_response.content))\n",
    "            img.save(img_filepath, format=\"PNG\")\n",
    "            \n",
    "            breed_info[breed_name] = img_filepath\n",
    "        else:\n",
    "            1\n",
    "            #print(\"No image\")\n",
    "        # end\n",
    "\n",
    "        count += 1\n",
    "        print(count, breed_name)\n",
    "    except:\n",
    "        1\n",
    "    # end\n",
    "    \n",
    "    # Only read several pages\n",
    "    if count >= 25:\n",
    "        break\n",
    "    # end\n",
    "# end\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
